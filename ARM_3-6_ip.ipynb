{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# over_sampling คือการทำค่า fail 1284 ให้เท่ากับค่า pass 271k เพื่อให้ data ในการ train เพิ่มมากขึ้น\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from flask import Flask, jsonify\n",
    "import json\n",
    "# from Database import create_sql_connection\n",
    "# from Database import create_sql_Component_Master\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import base64\n",
    "import datetime\n",
    "from flask import send_file, Flask, jsonify, request\n",
    "from flask_cors import CORS\n",
    "import pyodbc\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pymssql\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sql_connection():\n",
    "    try:\n",
    "        conn = pyodbc.connect(\n",
    "            'DRIVER={SQL Server};'\n",
    "            'SERVER=192.168.101.219;'\n",
    "            'DATABASE=DataforAnalysis;'\n",
    "            'UID=DATALYZER;'\n",
    "            'PWD=NMB54321'\n",
    "        )\n",
    "        \n",
    "        return conn\n",
    "    except pyodbc.Error as e:\n",
    "        print(\"Error connecting to SQL Server:\", e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(model, line, start, end, selecteKPOV, selecteKPIV):\n",
    "    conn = create_sql_connection()\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Define the SQL query with placeholders for parameters\n",
    "    query_template = \"\"\"\n",
    "        SELECT {selecteKPOV}, {selecteKPIV}\n",
    "        FROM [Diecast].[dbo].[Pivot]\n",
    "        JOIN [TransportData].[dbo].[Matching_Auto_Unit1] ON [Pivot].Diecast_S_N = [Matching_Auto_Unit1].Barcode_Base\n",
    "        JOIN [DataforAnalysis].[dbo].[DataML_Test] ON [DataML_Test].Barcode_motor = [Matching_Auto_Unit1].Barcode_Motor\n",
    "        WHERE [DataML_Test].[Model] = ? \n",
    "        AND [DataML_Test].[Line] = ? \n",
    "        AND [DataML_Test].[Date] BETWEEN ? AND ?\n",
    "        AND {conditions}\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure selecteKPIV is a list\n",
    "    if not isinstance(selecteKPIV, list):\n",
    "        selecteKPIV = [selecteKPIV]\n",
    "\n",
    "    # Create a comma-separated string of selecteKPIV for the SQL query\n",
    "    selecteKPIV_str = ', '.join(selecteKPIV)\n",
    "\n",
    "    # Create a list of conditions for each column in selecteKPIV\n",
    "    conditions = []\n",
    "    for column in selecteKPIV:\n",
    "        conditions.append(f\"{column} IS NOT NULL\")\n",
    "\n",
    "    # Combine the conditions using 'AND' and wrap them in parentheses\n",
    "    conditions_str = \" AND \".join(conditions)\n",
    "    conditions_str = f\"{conditions_str}\"\n",
    "\n",
    "    # Execute the query with parameters and fetch data\n",
    "    query = query_template.format(selecteKPOV=selecteKPOV, selecteKPIV=selecteKPIV_str, conditions=conditions_str)\n",
    "    datasets = pd.read_sql(query, conn, params=(model, line, start, end))\n",
    "\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_chartML(model, Line, start, end, selecteKPOV, selecteKPIV):\n",
    "    datasets = fetch_data(model, Line, start, end, selecteKPOV, selecteKPIV)\n",
    "    corrmat = datasets.corr()\n",
    "\n",
    "    f, ax = plt.subplots(figsize=(12, 9))\n",
    "    sns.heatmap(corrmat, vmax=.8, square=True, cmap='coolwarm', annot=True)\n",
    "\n",
    "    plt.savefig('../TrainingNodeJS/chart/heatmap.png')\n",
    "    plt.close()\n",
    "\n",
    "    return send_file(f\"../TrainingNodeJS/chart/heatmap.png\", mimetype='image/png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairplot(model, Line, start, end, selecteKPOV, selecteKPIV):\n",
    "    datasets = fetch_data(model, Line, start, end, selecteKPOV, selecteKPIV)\n",
    "    pairplot = sns.pairplot(datasets, height=4)\n",
    "\n",
    "    pairplot.savefig('../TrainingNodeJS/chart/pairplot.png')\n",
    "    plt.close()\n",
    "\n",
    "    return send_file(f\"../TrainingNodeJS/chart/pairplot.png\", mimetype='image/png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_describe(model, Line, start, end, selecteKPOV, selecteKPIV):\n",
    "    datasets = fetch_data(model, Line, start, end, selecteKPOV, selecteKPIV)\n",
    "    summary = datasets.describe(include='all')\n",
    "\n",
    "    # Convert the summary to a JSON format\n",
    "    summary_json = summary.to_json()\n",
    "    print(summary_json)\n",
    "\n",
    "    # If you want to return the JSON data\n",
    "    return summary_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BIN_KPOV(model, line, start, end, selecteKPOV, selecteKPIV, minKPOV, maxKPOV):\n",
    "    # Call the summary_describe function\n",
    "    data = fetch_data(model, line, start, end, selecteKPOV, selecteKPIV)\n",
    "    print( f\"BIN_KPOV_fetch_data\",{selecteKPIV})\n",
    "\n",
    "    # Modify the selected column as needed\n",
    "    data[selecteKPOV] = pd.cut(\n",
    "        data[selecteKPOV],\n",
    "        bins=[-np.inf, float(minKPOV), float(maxKPOV), np.inf],\n",
    "        labels=['fail_low', 'Pass', 'fail_high']\n",
    "    )\n",
    "\n",
    "    # Replace 'fail_low' and 'fail_high' with 'fail'\n",
    "    data[selecteKPOV].replace(['fail_low', 'fail_high'], 'fail', inplace=True)\n",
    "\n",
    "    # Count the occurrences of different values in the original data\n",
    "    count_before_smote = dict(Counter(data[selecteKPOV]))\n",
    "\n",
    "    k = 3\n",
    "    X = data.loc[:, data.columns != selecteKPOV]  # Use selecteKPOV here\n",
    "    y = data[selecteKPOV]  # Use selecteKPOV here\n",
    "\n",
    "    # Sampling strategy\n",
    "    sm = SMOTE(sampling_strategy='minority', k_neighbors=k, random_state=100)\n",
    "    X_res, y_res = sm.fit_resample(X, y)\n",
    "\n",
    "    # Concatenate the resampled data\n",
    "    datasets = pd.concat([pd.DataFrame(X_res), pd.DataFrame(y_res)], axis=1)\n",
    "\n",
    "    # Replace 'fail_low' and 'fail_high' with 'fail' in the resampled data\n",
    "    datasets[selecteKPOV].replace(['fail_low', 'fail_high'], 'fail', inplace=True)\n",
    "\n",
    "    # Count the occurrences of different values in the resampled data\n",
    "    count_after_smote = dict(Counter(datasets[selecteKPOV]))\n",
    "\n",
    "    # Convert the DataFrame to a JSON object\n",
    "    # json_data = datasets.to_json(orient='records')\n",
    "\n",
    "    # Create a dictionary to include the count results in the JSON response\n",
    "    response_data = {\n",
    "        # \"data\": json_data,\n",
    "        \"count_before_smote\": count_before_smote,\n",
    "        \"count_after_smote\": count_after_smote\n",
    "      \n",
    "    }\n",
    "    print(datasets)\n",
    "    return response_data,datasets\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_bin(model, selecteKPIV):\n",
    "    conn = create_sql_connection()\n",
    "    cursor = conn.cursor()\n",
    "    # สร้างรายการเพื่อเก็บ DataFrames สำหรับแต่ละ selecteKPIV\n",
    "    result_datasets = []\n",
    "    query = \"\"  # สร้างตัวแปร query ที่ระดับภายนอกของลูป for\n",
    "    for KPIV in selecteKPIV:\n",
    "        query_template = f\"\"\"\n",
    "        with set1 as (SELECT [id]\n",
    "          ,[Fullname]\n",
    "          ,[Model]\n",
    "          ,[Parameter]\n",
    "          ,[USL]\n",
    "          ,[LSL]\n",
    "          ,CL\n",
    "          ,[USL]-[LSL] as \"X\"\n",
    "          ,([USL]-[LSL])/6 as \"Y\"\n",
    "          ,[Part]\n",
    "          ,[Machine]\n",
    "          ,[empNumber]\n",
    "          ,[createdAt]\n",
    "          ,[updatedAt]\n",
    "      FROM [Component_Master].[dbo].[Master_matchings]\n",
    "      )\n",
    "      select \n",
    "        [Model]\n",
    "        ,[Parameter]\n",
    "        ,[LSL] - 0.0001 as \"LCL_3\"\n",
    "        ,[LSL] + 1 * y - 0.0001 as \"LCL_2\"\n",
    "        ,[LSL] + 2 * y - 0.0001 as \"LCL_1\"\n",
    "        ,[LSL] + 3 * y - 0.0001 as \"CL\"\n",
    "        ,[LSL] + 4 * y - 0.0001 as \"UCL_1\"\n",
    "        ,[LSL] + 5 * y - 0.0001 as \"UCL_2\"\n",
    "        ,[LSL] + 6 * y as \"UCL_3\"\n",
    "      from set1\n",
    "      where Model='{model}' and parameter = '{KPIV}'  -- ใช้ KPIV ที่เลือก\n",
    "        \"\"\"\n",
    "        query = query_template  # กำหนดค่าให้กับตัวแปร query\n",
    "        datasets_bin = pd.read_sql(query, conn)\n",
    "        result_datasets.append(datasets_bin)\n",
    "        print(f\"datasets_bin_query\", query)\n",
    "    # รวม DataFrames ในรายการเป็น DataFrame เดียวกัน\n",
    "    combined_dataframe = pd.concat(result_datasets, axis=0, ignore_index=True)\n",
    "\n",
    "    # แปลง DataFrame เป็น JSON\n",
    "    combined_json = combined_dataframe.to_json(orient='records')\n",
    "\n",
    "    # ส่ง JSON ผ่าน API response\n",
    "    return jsonify(combined_json),result_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DATASETS_BIN(model, line, start, end, selecteKPOV, selecteKPIV, minKPOV, maxKPOV):\n",
    "    conn = create_sql_connection()\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    for KPIV in selecteKPIV:\n",
    "        datasets = BIN_KPOV(model, line, start, end, selecteKPOV, selecteKPIV, minKPOV, maxKPOV)\n",
    "        result_datasets = data_bin(model, selecteKPIV)\n",
    "    # Call summary_describe to get summary information for the current KPIV\n",
    "        LCL_3 = result_datasets[KPIV]['LCL_3']\n",
    "        LCL_2 = result_datasets[KPIV]['LCL_2']\n",
    "        LCL_1 = result_datasets[KPIV]['LCL_1']\n",
    "        CL = result_datasets[KPIV]['CL']\n",
    "        UCL_1 = result_datasets[KPIV]['UCL_1']\n",
    "        UCL_2 = result_datasets[KPIV]['UCL_2']\n",
    "        UCL_3 = result_datasets[KPIV]['UCL_3']\n",
    "\n",
    "    # Print the statistics for the current KPIV\n",
    "    print(f\"{KPIV} Statistics:\")\n",
    "    print(f\"3LCL: {LCL_3}\")\n",
    "    print(f\"2LCL: {LCL_2}\")\n",
    "    print(f\"LCL: {LCL_1}\")\n",
    "    print(f\"CL: {CL}\")\n",
    "    print(f\"UCL: {UCL_1}\")\n",
    "    print(f\"2UCL: {UCL_2}\")\n",
    "    print(f\"3UCL: {UCL_3}\")\n",
    "\n",
    "    # Modify the corresponding column in the DataFrame\n",
    "    column_name = f'{KPIV}'  # Use KPIV as the new column name\n",
    "    datasets[column_name] = pd.cut(\n",
    "    pd.to_numeric(datasets[column_name], errors='coerce'),\n",
    "  \n",
    "        bins=[-np.inf,LCL_3, LCL_2, LCL_1, CL, UCL_1, UCL_2, UCL_3, np.inf],\n",
    "        labels=['-4', '-3', '-2', '-1', '1', '2', '3', '4'],\n",
    "        duplicates='drop'\n",
    "    )\n",
    "\n",
    "    response_data_json = json.dumps(response_data)\n",
    "    response_data = {\n",
    "    \"data\": response_data_json,\n",
    "\n",
    "    }\n",
    "    return response_data_json,response_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_bin(model, selecteKPIV):\n",
    "    conn = create_sql_connection()\n",
    "    cursor = conn.cursor()\n",
    "    print(f\"data_bin\",selecteKPIV)\n",
    "    # สร้างรายการเพื่อเก็บ DataFrames สำหรับแต่ละ selecteKPIV\n",
    "    \n",
    "    result_datasets = []\n",
    "    query = \"\"  # สร้างตัวแปร query ที่ระดับภายนอกของลูป for\n",
    "    for KPIV in selecteKPIV:\n",
    "            query_template = f\"\"\"\n",
    "        with set1 as (SELECT [id]\n",
    "          ,[Fullname]\n",
    "          ,[Model]\n",
    "          ,[Parameter]\n",
    "          ,[USL]\n",
    "          ,[LSL]\n",
    "          ,CL\n",
    "          ,[USL]-[LSL] as \"X\"\n",
    "          ,([USL]-[LSL])/6 as \"Y\"\n",
    "          ,[Part]\n",
    "          ,[Machine]\n",
    "          ,[empNumber]\n",
    "          ,[createdAt]\n",
    "          ,[updatedAt]\n",
    "      FROM [Component_Master].[dbo].[Master_matchings]\n",
    "      )\n",
    "      select \n",
    "        [Model]\n",
    "        ,[Parameter]\n",
    "        ,[LSL] - 0.0001 as \"LCL_3\"\n",
    "        ,[LSL] + 1 * y - 0.0001 as \"LCL_2\"\n",
    "        ,[LSL] + 2 * y - 0.0001 as \"LCL_1\"\n",
    "        ,[LSL] + 3 * y - 0.0001 as \"CL\"\n",
    "        ,[LSL] + 4 * y - 0.0001 as \"UCL_1\"\n",
    "        ,[LSL] + 5 * y - 0.0001 as \"UCL_2\"\n",
    "        ,[LSL] + 6 * y as \"UCL_3\"\n",
    "      from set1\n",
    "      where Model='{model}' and parameter = '{KPIV}'  -- ใช้ KPIV ที่เลือก\n",
    "        \"\"\"\n",
    "        \n",
    "            query = query_template  # กำหนดค่าให้กับตัวแปร query\n",
    "            datasets_bin = pd.read_sql(query, conn)\n",
    "            result_datasets.append(datasets_bin)\n",
    "            print(f\"datasets_bin_query\", query)\n",
    "    # รวม DataFrames ในรายการเป็น DataFrame เดียวกัน\n",
    "            combined_dataframe = pd.concat(result_datasets, axis=0, ignore_index=True)\n",
    "\n",
    "    # แปลง DataFrame เป็น JSON\n",
    "            combined_json = combined_dataframe.to_json(orient='records')\n",
    "\n",
    "    # ส่ง JSON ผ่าน API response\n",
    "    return jsonify(combined_json),result_datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def data_bin(model, line, start, end, selecteKPOV, selecteKPIV, minKPOV, maxKPOV):\n",
    "    conn = create_sql_connection()\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # สร้างรายการเพื่อเก็บ DataFrames สำหรับแต่ละ selecteKPIV\n",
    "    result_datasets = []\n",
    "    \n",
    "    for KPIV in selecteKPIV:\n",
    "        query_template = f\"\"\"\n",
    "        with set1 as (SELECT [id]\n",
    "          ,[Fullname]\n",
    "          ,[Model]\n",
    "          ,[Parameter]\n",
    "          ,[USL]\n",
    "          ,[LSL]\n",
    "          ,CL\n",
    "          ,[USL]-[LSL] as \"X\"\n",
    "          ,([USL]-[LSL])/6 as \"Y\"\n",
    "          ,[Part]\n",
    "          ,[Machine]\n",
    "          ,[empNumber]\n",
    "          ,[createdAt]\n",
    "          ,[updatedAt]\n",
    "      FROM [Component_Master].[dbo].[Master_matchings]\n",
    "      )\n",
    "      select \n",
    "        [Model]\n",
    "        ,[Parameter]\n",
    "        ,[LSL] - 0.0001 as \"LCL_3\"\n",
    "        ,[LSL] + 1 * y - 0.0001 as \"LCL_2\"\n",
    "        ,[LSL] + 2 * y - 0.0001 as \"LCL_1\"\n",
    "        ,[LSL] + 3 * y - 0.0001 as \"CL\"\n",
    "        ,[LSL] + 4 * y - 0.0001 as \"UCL_1\"\n",
    "        ,[LSL] + 5 * y - 0.0001 as \"UCL_2\"\n",
    "        ,[LSL] + 6 * y as \"UCL_3\"\n",
    "      from set1\n",
    "      where Model='{model}' and parameter = '{KPIV}'\n",
    "        \"\"\"\n",
    "        \n",
    "        query = query_template\n",
    "        datasets_bin = pd.read_sql(query, conn)\n",
    "        result_datasets.append(datasets_bin)\n",
    "\n",
    "    for KPIV in selecteKPIV:\n",
    "    # Call summary_describe to get summary information for the current KPIV\n",
    "        LCL_3 = datasets_bin[KPIV]['LCL_3']\n",
    "        LCL_2 = datasets_bin[KPIV]['LCL_2']\n",
    "        LCL_1 = datasets_bin[KPIV]['LCL_1']\n",
    "        CL = datasets_bin[KPIV]['CL']\n",
    "        UCL_1 = datasets_bin[KPIV]['UCL_1']\n",
    "        UCL_2 = datasets_bin[KPIV]['UCL_2']\n",
    "        UCL_3 = datasets_bin[KPIV]['UCL_3']\n",
    "\n",
    "    # Print the statistics for the current KPIV\n",
    "    print(f\"{KPIV} Statistics:\")\n",
    "    print(f\"3LCL: {LCL_3}\")\n",
    "    print(f\"2LCL: {LCL_2}\")\n",
    "    print(f\"LCL: {LCL_1}\")\n",
    "    print(f\"CL: {CL}\")\n",
    "    print(f\"UCL: {UCL_1}\")\n",
    "    print(f\"2UCL: {UCL_2}\")\n",
    "    print(f\"3UCL: {UCL_3}\")\n",
    "\n",
    "    # Modify the corresponding column in the DataFrame\n",
    "    column_name = f'{KPIV}'  # Use KPIV as the new column name\n",
    "    result_datasets[column_name] = pd.cut(\n",
    "    pd.to_numeric(result_datasets[column_name], errors='coerce'),\n",
    "  \n",
    "        bins=[-np.inf,LCL_3, LCL_2, LCL_1, CL, UCL_1, UCL_2, UCL_3, np.inf],\n",
    "        labels=['-4', '-3', '-2', '-1', '1', '2', '3', '4'],\n",
    "        duplicates='drop'\n",
    "    )\n",
    "\n",
    "    response_data_json = json.dumps(response_data)\n",
    "    response_data = {\n",
    "    \"data\": response_data_json,\n",
    "\n",
    "    }\n",
    "    return response_data_json,response_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IT\\AppData\\Local\\Temp\\ipykernel_14404\\1780064111.py:35: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  datasets = pd.read_sql(query, conn, params=(model, line, start, end))\n"
     ]
    },
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql '\n        SELECT Projection1, Parallelism_Stack, Set_Dimension_Stack, P1_Attractive_1\n        FROM [Diecast].[dbo].[Pivot]\n        JOIN [TransportData].[dbo].[Matching_Auto_Unit1] ON [Pivot].Diecast_S_N = [Matching_Auto_Unit1].Barcode_Base\n        JOIN [DataforAnalysis].[dbo].[DataML_Test] ON [DataML_Test].Barcode_motor = [Matching_Auto_Unit1].Barcode_Motor\n        WHERE [DataML_Test].[Model] = ? \n        AND [DataML_Test].[Line] = ? \n        AND [DataML_Test].[Date] BETWEEN ? AND ?\n        AND Parallelism_Stack IS NOT NULL AND Set_Dimension_Stack IS NOT NULL AND P1_Attractive_1 IS NOT NULL\n    ': ('42S22', \"[42S22] [Microsoft][ODBC SQL Server Driver][SQL Server]Invalid column name 'Parallelism_Stack'. (207) (SQLExecDirectW); [42S22] [Microsoft][ODBC SQL Server Driver][SQL Server]Invalid column name 'Set_Dimension_Stack'. (207); [42S22] [Microsoft][ODBC SQL Server Driver][SQL Server]Invalid column name 'P1_Attractive_1'. (207); [42S22] [Microsoft][ODBC SQL Server Driver][SQL Server]Invalid column name 'Parallelism_Stack'. (207); [42S22] [Microsoft][ODBC SQL Server Driver][SQL Server]Invalid column name 'Set_Dimension_Stack'. (207); [42S22] [Microsoft][ODBC SQL Server Driver][SQL Server]Invalid column name 'P1_Attractive_1'. (207); [42S22] [Microsoft][ODBC SQL Server Driver][SQL Server]Statement(s) could not be prepared. (8180)\")",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mProgrammingError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:2018\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2017\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 2018\u001b[0m     cur\u001b[39m.\u001b[39mexecute(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   2019\u001b[0m     \u001b[39mreturn\u001b[39;00m cur\n",
      "\u001b[1;31mProgrammingError\u001b[0m: ('42S22', \"[42S22] [Microsoft][ODBC SQL Server Driver][SQL Server]Invalid column name 'Parallelism_Stack'. (207) (SQLExecDirectW); [42S22] [Microsoft][ODBC SQL Server Driver][SQL Server]Invalid column name 'Set_Dimension_Stack'. (207); [42S22] [Microsoft][ODBC SQL Server Driver][SQL Server]Invalid column name 'P1_Attractive_1'. (207); [42S22] [Microsoft][ODBC SQL Server Driver][SQL Server]Invalid column name 'Parallelism_Stack'. (207); [42S22] [Microsoft][ODBC SQL Server Driver][SQL Server]Invalid column name 'Set_Dimension_Stack'. (207); [42S22] [Microsoft][ODBC SQL Server Driver][SQL Server]Invalid column name 'P1_Attractive_1'. (207); [42S22] [Microsoft][ODBC SQL Server Driver][SQL Server]Statement(s) could not be prepared. (8180)\")",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32me:\\ML\\src\\ARM_3-6_ip.ipynb Cell 13\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/ML/src/ARM_3-6_ip.ipynb#X15sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m response_data_json,response_data\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/ML/src/ARM_3-6_ip.ipynb#X15sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39m# Call BIN_KPOV_KPIV with the provided parameters and preprocessed_data\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/ML/src/ARM_3-6_ip.ipynb#X15sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m result_datasets, response_data_json, response_data \u001b[39m=\u001b[39m BIN_KPIV(\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/ML/src/ARM_3-6_ip.ipynb#X15sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mLONGSP\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m3-6\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m2023-08-01\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m2023-09-14\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mProjection1\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/ML/src/ARM_3-6_ip.ipynb#X15sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     [\u001b[39m'\u001b[39;49m\u001b[39mParallelism_Stack\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mSet_Dimension_Stack\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mP1_Attractive_1\u001b[39;49m\u001b[39m'\u001b[39;49m], \u001b[39m0.4648\u001b[39;49m, \u001b[39m0.5664\u001b[39;49m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/ML/src/ARM_3-6_ip.ipynb#X15sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m )\n",
      "\u001b[1;32me:\\ML\\src\\ARM_3-6_ip.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/ML/src/ARM_3-6_ip.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mBIN_KPIV\u001b[39m(model, line, start, end, selecteKPOV, selecteKPIV, minKPOV, maxKPOV):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/ML/src/ARM_3-6_ip.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     data \u001b[39m=\u001b[39m BIN_KPOV(model, line, start, end, selecteKPOV, selecteKPIV, minKPOV, maxKPOV)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/ML/src/ARM_3-6_ip.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     response_data \u001b[39m=\u001b[39m {}   \n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/ML/src/ARM_3-6_ip.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mfor\u001b[39;00m KPIV \u001b[39min\u001b[39;00m selecteKPIV:\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/ML/src/ARM_3-6_ip.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m# Call summary_describe to get summary information for the current KPIV\u001b[39;00m\n",
      "\u001b[1;32me:\\ML\\src\\ARM_3-6_ip.ipynb Cell 13\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/ML/src/ARM_3-6_ip.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mBIN_KPOV\u001b[39m(model, line, start, end, selecteKPOV, selecteKPIV, minKPOV, maxKPOV):\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/ML/src/ARM_3-6_ip.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m# Call the summary_describe function\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/ML/src/ARM_3-6_ip.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     data \u001b[39m=\u001b[39m fetch_data(model, line, start, end, selecteKPOV, selecteKPIV)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/ML/src/ARM_3-6_ip.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mprint\u001b[39m( \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBIN_KPOV_fetch_data\u001b[39m\u001b[39m\"\u001b[39m,{selecteKPIV})\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/ML/src/ARM_3-6_ip.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m# Modify the selected column as needed\u001b[39;00m\n",
      "\u001b[1;32me:\\ML\\src\\ARM_3-6_ip.ipynb Cell 13\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/ML/src/ARM_3-6_ip.ipynb#X15sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# Execute the query with parameters and fetch data\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/ML/src/ARM_3-6_ip.ipynb#X15sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m query \u001b[39m=\u001b[39m query_template\u001b[39m.\u001b[39mformat(selecteKPOV\u001b[39m=\u001b[39mselecteKPOV, selecteKPIV\u001b[39m=\u001b[39mselecteKPIV_str, conditions\u001b[39m=\u001b[39mconditions_str)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/ML/src/ARM_3-6_ip.ipynb#X15sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m datasets \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_sql(query, conn, params\u001b[39m=\u001b[39;49m(model, line, start, end))\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/ML/src/ARM_3-6_ip.ipynb#X15sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mreturn\u001b[39;00m datasets\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:564\u001b[0m, in \u001b[0;36mread_sql\u001b[1;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize)\u001b[0m\n\u001b[0;32m    561\u001b[0m pandas_sql \u001b[39m=\u001b[39m pandasSQL_builder(con)\n\u001b[0;32m    563\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(pandas_sql, SQLiteDatabase):\n\u001b[1;32m--> 564\u001b[0m     \u001b[39mreturn\u001b[39;00m pandas_sql\u001b[39m.\u001b[39;49mread_query(\n\u001b[0;32m    565\u001b[0m         sql,\n\u001b[0;32m    566\u001b[0m         index_col\u001b[39m=\u001b[39;49mindex_col,\n\u001b[0;32m    567\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    568\u001b[0m         coerce_float\u001b[39m=\u001b[39;49mcoerce_float,\n\u001b[0;32m    569\u001b[0m         parse_dates\u001b[39m=\u001b[39;49mparse_dates,\n\u001b[0;32m    570\u001b[0m         chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[0;32m    571\u001b[0m     )\n\u001b[0;32m    573\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    574\u001b[0m     _is_table_name \u001b[39m=\u001b[39m pandas_sql\u001b[39m.\u001b[39mhas_table(sql)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:2078\u001b[0m, in \u001b[0;36mSQLiteDatabase.read_query\u001b[1;34m(self, sql, index_col, coerce_float, params, parse_dates, chunksize, dtype)\u001b[0m\n\u001b[0;32m   2066\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_query\u001b[39m(\n\u001b[0;32m   2067\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   2068\u001b[0m     sql,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2074\u001b[0m     dtype: DtypeArg \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   2075\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Iterator[DataFrame]:\n\u001b[0;32m   2077\u001b[0m     args \u001b[39m=\u001b[39m _convert_params(sql, params)\n\u001b[1;32m-> 2078\u001b[0m     cursor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m   2079\u001b[0m     columns \u001b[39m=\u001b[39m [col_desc[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m col_desc \u001b[39min\u001b[39;00m cursor\u001b[39m.\u001b[39mdescription]\n\u001b[0;32m   2081\u001b[0m     \u001b[39mif\u001b[39;00m chunksize \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:2030\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2027\u001b[0m     \u001b[39mraise\u001b[39;00m ex \u001b[39mfrom\u001b[39;00m \u001b[39minner_exc\u001b[39;00m\n\u001b[0;32m   2029\u001b[0m ex \u001b[39m=\u001b[39m DatabaseError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExecution failed on sql \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00margs[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mexc\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 2030\u001b[0m \u001b[39mraise\u001b[39;00m ex \u001b[39mfrom\u001b[39;00m \u001b[39mexc\u001b[39;00m\n",
      "\u001b[1;31mDatabaseError\u001b[0m: Execution failed on sql '\n        SELECT Projection1, Parallelism_Stack, Set_Dimension_Stack, P1_Attractive_1\n        FROM [Diecast].[dbo].[Pivot]\n        JOIN [TransportData].[dbo].[Matching_Auto_Unit1] ON [Pivot].Diecast_S_N = [Matching_Auto_Unit1].Barcode_Base\n        JOIN [DataforAnalysis].[dbo].[DataML_Test] ON [DataML_Test].Barcode_motor = [Matching_Auto_Unit1].Barcode_Motor\n        WHERE [DataML_Test].[Model] = ? \n        AND [DataML_Test].[Line] = ? \n        AND [DataML_Test].[Date] BETWEEN ? AND ?\n        AND Parallelism_Stack IS NOT NULL AND Set_Dimension_Stack IS NOT NULL AND P1_Attractive_1 IS NOT NULL\n    ': ('42S22', \"[42S22] [Microsoft][ODBC SQL Server Driver][SQL Server]Invalid column name 'Parallelism_Stack'. (207) (SQLExecDirectW); [42S22] [Microsoft][ODBC SQL Server Driver][SQL Server]Invalid column name 'Set_Dimension_Stack'. (207); [42S22] [Microsoft][ODBC SQL Server Driver][SQL Server]Invalid column name 'P1_Attractive_1'. (207); [42S22] [Microsoft][ODBC SQL Server Driver][SQL Server]Invalid column name 'Parallelism_Stack'. (207); [42S22] [Microsoft][ODBC SQL Server Driver][SQL Server]Invalid column name 'Set_Dimension_Stack'. (207); [42S22] [Microsoft][ODBC SQL Server Driver][SQL Server]Invalid column name 'P1_Attractive_1'. (207); [42S22] [Microsoft][ODBC SQL Server Driver][SQL Server]Statement(s) could not be prepared. (8180)\")"
     ]
    }
   ],
   "source": [
    "def BIN_KPIV(model, line, start, end, selecteKPOV, selecteKPIV, minKPOV, maxKPOV):\n",
    "    data = BIN_KPOV(model, line, start, end, selecteKPOV, selecteKPIV, minKPOV, maxKPOV)\n",
    "    response_data = {}   \n",
    "    for KPIV in selecteKPIV:\n",
    "    # Call summary_describe to get summary information for the current KPIV\n",
    "        datasets_bin = data_bin(model)\n",
    "        \n",
    "    LCL_3 = datasets_bin[KPIV]['LCL_3']\n",
    "    LCL_2 = datasets_bin[KPIV]['LCL_2']\n",
    "    LCL_1 = datasets_bin[KPIV]['LCL_1']\n",
    "    CL = datasets_bin[KPIV]['CL']\n",
    "    UCL_1 = datasets_bin[KPIV]['UCL_1']\n",
    "    UCL_2 = datasets_bin[KPIV]['UCL_2']\n",
    "    UCL_3 = datasets_bin[KPIV]['UCL_3']\n",
    "\n",
    "    # Print the statistics for the current KPIV\n",
    "    print(f\"{KPIV} Statistics:\")\n",
    "    print(f\"3LCL: {LCL_3}\")\n",
    "    print(f\"2LCL: {LCL_2}\")\n",
    "    print(f\"LCL: {LCL_1}\")\n",
    "    print(f\"CL: {CL}\")\n",
    "    print(f\"UCL: {UCL_1}\")\n",
    "    print(f\"2UCL: {UCL_2}\")\n",
    "    print(f\"3UCL: {UCL_3}\")\n",
    "\n",
    "    # Modify the corresponding column in the DataFrame\n",
    "    column_name = f'{KPIV}'  # Use KPIV as the new column name\n",
    "    data[column_name] = pd.cut(\n",
    "        pd.to_numeric(data[column_name], errors='coerce'),\n",
    "        bins=[-np.inf, LCL_3, LCL_2, LCL_1, CL, UCL_1, UCL_2, UCL_3, np.inf],\n",
    "        labels=['-3', '-2', '-1', '0', '1', '2', '3', '4'],\n",
    "        duplicates='drop'\n",
    "    )\n",
    "\n",
    "    # Store the statistics in response_data\n",
    "    response_data[KPIV] = {\n",
    "        \"3LCL\": LCL_3,\n",
    "        \"2LCL\": LCL_2,\n",
    "        \"LCL\": LCL_1,\n",
    "        \"CL\": CL,\n",
    "        \"UCL\": UCL_1,\n",
    "        \"2UCL\": UCL_2,\n",
    "        \"3UCL\": UCL_3\n",
    "    }\n",
    "    print(response_data)\n",
    "    response_data_json = json.dumps(response_data)\n",
    "    response_data = {\n",
    "    \"data\": response_data_json,\n",
    "\n",
    "    }\n",
    "    return response_data_json,response_data\n",
    "    \n",
    "# Call BIN_KPOV_KPIV with the provided parameters and preprocessed_data\n",
    "result_datasets, response_data_json, response_data = BIN_KPIV(\n",
    "    'LONGSP', '3-6', '2023-08-01', '2023-09-14', 'Projection1',\n",
    "    ['Parallelism_Stack', 'Set_Dimension_Stack','P1_Attractive_1'], 0.4648, 0.5664\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def BIN_KPOV(model, line, start, end, selecteKPOV, selecteKPIV, minKPOV, maxKPOV):\n",
    "    # Call the summary_describe function\n",
    "    data = fetch_data(model, line, start, end, selecteKPOV, selecteKPIV)\n",
    "\n",
    "    # Modify the selected column as needed\n",
    "    data[selecteKPOV] = pd.cut(\n",
    "        data[selecteKPOV],\n",
    "        bins=[-np.inf, float(minKPOV), float(maxKPOV), np.inf],\n",
    "        labels=['fail_low', 'Pass', 'fail_high']\n",
    "    )\n",
    "\n",
    "    # Replace 'fail_low' and 'fail_high' with 'fail'\n",
    "    data[selecteKPOV].replace(['fail_low', 'fail_high'], 'fail', inplace=True)\n",
    "\n",
    "    # Count the occurrences of different values in the original data\n",
    "    count_before_smote = dict(Counter(data[selecteKPOV]))\n",
    "\n",
    "    k = 3\n",
    "    X = data.loc[:, data.columns != selecteKPOV]  # Use selecteKPOV here\n",
    "    y = data[selecteKPOV]  # Use selecteKPOV here\n",
    "\n",
    "    # Sampling strategy\n",
    "    sm = SMOTE(sampling_strategy='minority', k_neighbors=k, random_state=100)\n",
    "    X_res, y_res = sm.fit_resample(X, y)\n",
    "\n",
    "    # Concatenate the resampled data\n",
    "    datasets = pd.concat([pd.DataFrame(X_res), pd.DataFrame(y_res)], axis=1)\n",
    "\n",
    "    # Replace 'fail_low' and 'fail_high' with 'fail' in the resampled data\n",
    "    datasets[selecteKPOV].replace(['fail_low', 'fail_high'], 'fail', inplace=True)\n",
    "\n",
    "    # Count the occurrences of different values in the resampled data\n",
    "    count_after_smote = dict(Counter(datasets[selecteKPOV]))\n",
    "\n",
    "    # Initialize dictionaries to store AVG and Std for each selecteKPIV\n",
    "    response_data = {\n",
    "    \"data\": response_data_json,\n",
    "    \"count_before_smote\": count_before_smote,\n",
    "    \"count_after_smote\": count_after_smote\n",
    "    }\n",
    "    \n",
    "\n",
    "    # Iterate over each KPIV in selecteKPIV\n",
    "\n",
    "    return datasets.dropna(), response_data_json,response_data\n",
    "\n",
    "# Call BIN_KPOV_KPIV with the provided parameters and preprocessed_data\n",
    "result_datasets, response_data_json, response_data = BIN_KPOV(\n",
    "    'LONGSP', '3-6', '2023-08-01', '2023-09-14', 'Projection1',\n",
    "    ['Parallelism_Stack', 'Set_Dimension_Stack','P1_Attractive_1'], 0.4648, 0.5664\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BIN_KPIV(model, line, start, end, selecteKPOV, selecteKPIV, minKPOV, maxKPOV):\n",
    "    data = fetch_data(model, line, start, end, selecteKPOV, selecteKPIV, minKPOV, maxKPOV)\n",
    "    response_data = {}   \n",
    "    for KPIV in selecteKPIV:\n",
    "    # Call summary_describe to get summary information for the current KPIV\n",
    "        datasets_bin = data_bin(model)\n",
    "        \n",
    "    LCL_3 = datasets_bin[KPIV]['LCL_-3']\n",
    "    LCL_2 = datasets_bin[KPIV]['LCL_-2']\n",
    "    LCL_1 = datasets_bin[KPIV]['LCL_-1']\n",
    "    CL = datasets_bin[KPIV]['CL']\n",
    "    UCL_1 = datasets_bin[KPIV]['UCL_1']\n",
    "    UCL_2 = datasets_bin[KPIV]['UCL_2']\n",
    "    UCL_3 = datasets_bin[KPIV]['UCL_3']\n",
    "\n",
    "    # Print the statistics for the current KPIV\n",
    "    print(f\"{KPIV} Statistics:\")\n",
    "    print(f\"3LCL: {LCL_3}\")\n",
    "    print(f\"2LCL: {LCL_2}\")\n",
    "    print(f\"LCL: {LCL_1}\")\n",
    "    print(f\"CL: {CL}\")\n",
    "    print(f\"UCL: {UCL_1}\")\n",
    "    print(f\"2UCL: {UCL_2}\")\n",
    "    print(f\"3UCL: {UCL_3}\")\n",
    "\n",
    "    # Modify the corresponding column in the DataFrame\n",
    "    column_name = f'{KPIV}'  # Use KPIV as the new column name\n",
    "    data[column_name] = pd.cut(\n",
    "        pd.to_numeric(data[column_name], errors='coerce'),\n",
    "        bins=[-np.inf, LCL_3, LCL_2, LCL_1, CL, UCL_1, UCL_2, UCL_3, np.inf],\n",
    "        labels=['-3', '-2', '-1', '0', '1', '2', '3', '4'],\n",
    "        duplicates='drop'\n",
    "    )\n",
    "\n",
    "    # Store the statistics in response_data\n",
    "    response_data[KPIV] = {\n",
    "        \"3LCL\": LCL_3,\n",
    "        \"2LCL\": LCL_2,\n",
    "        \"LCL\": LCL_1,\n",
    "        \"CL\": CL,\n",
    "        \"UCL\": UCL_1,\n",
    "        \"2UCL\": UCL_2,\n",
    "        \"3UCL\": UCL_3\n",
    "    }\n",
    "    \n",
    "\n",
    "    print(response_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def BIN_KPOV_KPIV(model, line, start, end, selecteKPOV, selecteKPIV, minKPOV, maxKPOV):\n",
    "    # Call the summary_describe function\n",
    "    data = fetch_data(model, line, start, end, selecteKPOV, selecteKPIV)\n",
    "\n",
    "    # Modify the selected column as needed\n",
    "    data[selecteKPOV] = pd.cut(\n",
    "        data[selecteKPOV],\n",
    "        bins=[-np.inf, float(minKPOV), float(maxKPOV), np.inf],\n",
    "        labels=['fail_low', 'Pass', 'fail_high']\n",
    "    )\n",
    "\n",
    "    # Replace 'fail_low' and 'fail_high' with 'fail'\n",
    "    data[selecteKPOV].replace(['fail_low', 'fail_high'], 'fail', inplace=True)\n",
    "\n",
    "    # Count the occurrences of different values in the original data\n",
    "    count_before_smote = dict(Counter(data[selecteKPOV]))\n",
    "\n",
    "    k = 3\n",
    "    X = data.loc[:, data.columns != selecteKPOV]  # Use selecteKPOV here\n",
    "    y = data[selecteKPOV]  # Use selecteKPOV here\n",
    "\n",
    "    # Sampling strategy\n",
    "    sm = SMOTE(sampling_strategy='minority', k_neighbors=k, random_state=100)\n",
    "    X_res, y_res = sm.fit_resample(X, y)\n",
    "\n",
    "    # Concatenate the resampled data\n",
    "    datasets = pd.concat([pd.DataFrame(X_res), pd.DataFrame(y_res)], axis=1)\n",
    "\n",
    "    # Replace 'fail_low' and 'fail_high' with 'fail' in the resampled data\n",
    "    datasets[selecteKPOV].replace(['fail_low', 'fail_high'], 'fail', inplace=True)\n",
    "\n",
    "    # Count the occurrences of different values in the resampled data\n",
    "    count_after_smote = dict(Counter(datasets[selecteKPOV]))\n",
    "\n",
    "    # Initialize dictionaries to store AVG and Std for each selecteKPIV\n",
    "    AVG_dict = {}\n",
    "    Std_dict = {}\n",
    "    response_data = {}\n",
    "\n",
    "    # Iterate over each KPIV in selecteKPIV\n",
    "    for KPIV in selecteKPIV:\n",
    "        # Call summary_describe to get summary information for the current KPIV\n",
    "        summary_json = json.loads(summary_describe(model, line, start, end, selecteKPOV, KPIV))\n",
    "\n",
    "        \n",
    "        # Extract 'mean' and 'std' for the current KPIV\n",
    "        AVG = summary_json[KPIV]['mean']\n",
    "        Std = summary_json[KPIV]['std']\n",
    "\n",
    "        # Store AVG and Std in dictionaries\n",
    "        AVG_dict[KPIV] = AVG\n",
    "        Std_dict[KPIV] = Std\n",
    "\n",
    "        # Print the statistics for the current KPIV\n",
    "        print(f\"{KPIV} Statistics:\")\n",
    "        print(f\"AVG: {AVG}\")\n",
    "        print(f\"Std: {Std}\")\n",
    "        print(f\"3LCL: {AVG - 3 * Std}\")\n",
    "        print(f\"2LCL: {AVG - 2 * Std}\")\n",
    "        print(f\"LCL: {AVG - 1 * Std}\")\n",
    "        print(f\"CL: {AVG}\")\n",
    "        print(f\"UCL: {AVG + 1 * Std}\")\n",
    "        print(f\"2UCL: {AVG + 2 * Std}\")\n",
    "        print(f\"3UCL: {AVG + 3 * Std}\")\n",
    "\n",
    "        # Modify the corresponding column in the DataFrame\n",
    "        column_name = f'{KPIV}'  # Use KPIV as the new column name\n",
    "        data[column_name] = pd.cut(\n",
    "            pd.to_numeric(data[column_name], errors='coerce'),\n",
    "            bins=[-np.inf, AVG - 3 * Std, AVG - 2 * Std, AVG - Std, AVG, AVG + Std, AVG + 2 * Std, AVG + 3 * Std, np.inf],\n",
    "            labels=['-3', '-2', '-1', '0', '1', '2', '3', '4'],\n",
    "            duplicates='drop'\n",
    "        )\n",
    "\n",
    "        # Store the statistics in response_data\n",
    "        response_data[KPIV] = {\n",
    "            \"AVG\": AVG,\n",
    "            \"Std\": Std,\n",
    "            \"3LCL\": AVG - 3 * Std,\n",
    "            \"2LCL\": AVG - 2 * Std,\n",
    "            \"LCL\": AVG - 1 * Std,\n",
    "            \"CL\": AVG,\n",
    "            \"UCL\": AVG + 1 * Std,\n",
    "            \"2UCL\": AVG + 2 * Std,\n",
    "            \"3UCL\": AVG + 3 * Std\n",
    "        }\n",
    "\n",
    "    # Convert response_data to JSON\n",
    "    response_data_json = json.dumps(response_data)\n",
    "    \n",
    "    response_data = {\n",
    "    \"data\": response_data_json,\n",
    "    \"count_before_smote\": count_before_smote,\n",
    "    \"count_after_smote\": count_after_smote\n",
    "    }\n",
    "\n",
    "    return datasets.dropna(), response_data_json,response_data\n",
    "\n",
    "# Call BIN_KPOV_KPIV with the provided parameters and preprocessed_data\n",
    "result_datasets, response_data_json, response_data = BIN_KPOV(\n",
    "    'LONGSP', '3-6', '2023-08-01', '2023-09-14', 'Projection1',\n",
    "    ['Parallelism_Stack', 'Set_Dimension_Stack','P1_Attractive_1'], 0.4648, 0.5664\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BIN_KPOV(model, line, start, end, selecteKPOV, selecteKPIV, minKPOV, maxKPOV):\n",
    "    # Call the summary_describe function\n",
    "    data = fetch_data(model, line, start, end, selecteKPOV, selecteKPIV)\n",
    "\n",
    "    # Modify the selected column as needed\n",
    "    data[selecteKPOV] = pd.cut(\n",
    "        data[selecteKPOV],\n",
    "        bins=[-np.inf, float(minKPOV), float(maxKPOV), np.inf],\n",
    "        labels=['fail_low', 'Pass', 'fail_high']\n",
    "    )\n",
    "\n",
    "    # Replace 'fail_low' and 'fail_high' with 'fail'\n",
    "    data[selecteKPOV].replace(['fail_low', 'fail_high'], 'fail', inplace=True)\n",
    "\n",
    "    # Count the occurrences of different values in the original data\n",
    "    count_before_smote = dict(Counter(data[selecteKPOV]))\n",
    "\n",
    "    k = 3\n",
    "    X = data.loc[:, data.columns != selecteKPOV]  # Use selecteKPOV here\n",
    "    y = data[selecteKPOV]  # Use selecteKPOV here\n",
    "\n",
    "    # Sampling strategy\n",
    "    sm = SMOTE(sampling_strategy='minority', k_neighbors=k, random_state=100)\n",
    "    X_res, y_res = sm.fit_resample(X, y)\n",
    "\n",
    "    # Concatenate the resampled data\n",
    "    datasets = pd.concat([pd.DataFrame(X_res), pd.DataFrame(y_res)], axis=1)\n",
    "\n",
    "    # Replace 'fail_low' and 'fail_high' with 'fail' in the resampled data\n",
    "    datasets[selecteKPOV].replace(['fail_low', 'fail_high'], 'fail', inplace=True)\n",
    "\n",
    "    # Count the occurrences of different values in the resampled data\n",
    "    count_after_smote = dict(Counter(datasets[selecteKPOV]))\n",
    "\n",
    "    # Convert the DataFrame to a JSON object\n",
    "    json_data = datasets.to_json(orient='records')\n",
    "\n",
    "    # Create a dictionary to include the count results in the JSON response\n",
    "    response_data = {\n",
    "        \"data\": json_data,\n",
    "        \"count_before_smote\": count_before_smote,\n",
    "        \"count_after_smote\": count_after_smote\n",
    "      \n",
    "    }\n",
    "    return response_data,datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BIN_KPIV(model, line, start, end, selecteKPOV, selecteKPIV, minKPOV, maxKPOV):\n",
    "    # Call BIN_KPOV to get the initial data\n",
    "    initial_data = BIN_KPOV(model, line, start, end, selecteKPOV, selecteKPIV, minKPOV, maxKPOV)\n",
    "\n",
    "    # Convert the JSON data from BIN_KPOV back to a DataFrame\n",
    "    data_df = pd.read_json(initial_data[\"data\"])\n",
    "\n",
    "    # Continue with your BIN_KPIV logic to further process data_df\n",
    "\n",
    "    # Initialize dictionaries to store AVG and Std for each selecteKPIV\n",
    "    AVG_dict = {}\n",
    "    Std_dict = {}\n",
    "    \n",
    "    # Iterate over each KPIV in selecteKPIV\n",
    "    for KPIV in selecteKPIV:\n",
    "        # Call summary_describe to get summary information for the current KPIV\n",
    "        summary_json = json.loads(data_bin(model, line, start, end, selecteKPOV, KPIV))\n",
    "        \n",
    "        # Extract 'mean' and 'std' for the current KPIV\n",
    "        AVG = summary_json[KPIV]['mean']\n",
    "        Std = summary_json[KPIV]['std']\n",
    "        \n",
    "        # Store AVG and Std in dictionaries\n",
    "        AVG_dict[KPIV] = AVG  \n",
    "        Std_dict[KPIV] = Std\n",
    "    \n",
    "        print(AVG_dict[KPIV])\n",
    "        print(Std_dict[KPIV])\n",
    "\n",
    "        KPIV_3LCL = AVG - 3 * Std\n",
    "        KPIV_2LCL = AVG - 2 * Std\n",
    "        KPIV_LCL = AVG - 1 * Std\n",
    "        KPIV_CL = AVG \n",
    "        KPIV_UCL = AVG + 1 * Std\n",
    "        KPIV_2UCL = AVG + 2 * Std\n",
    "        KPIV_3UCL = AVG + 3 * Std\n",
    " \n",
    "        # Modify the corresponding 'Datum_probe' column\n",
    "        column_name = f'{KPIV}'  # ใช้ชื่อ KPIV เป็นชื่อคอลัมน์ใหม่\n",
    "        data_df[column_name] = pd.cut(\n",
    "        pd.to_numeric(data_df[column_name], errors='coerce'),  # แปลงค่าในคอลัมน์เป็นตัวเลขและจัดการกับข้อผิดพลาด\n",
    "        bins=[-np.inf, KPIV_3LCL, KPIV_2LCL, KPIV_LCL, KPIV_CL, KPIV_UCL, KPIV_2UCL, KPIV_3UCL, np.inf],  # กำหนดขอบของช่วง\n",
    "        labels=['-4','-3','-2','-1','1','2','3','4'],  # กำหนดรหัสสำหรับแต่ละช่วง\n",
    "        duplicates='drop'  # ตัวเลือกที่ระบุว่าจะลบข้อมูลที่ซ้ำกัน\n",
    "    \n",
    "\n",
    "    )\n",
    "   \n",
    "    return data_df.dropna()\n",
    "\n",
    "result_datasets = BIN_KPIV(\n",
    "    'LONGSP', '3-6', '2023-08-01', '2023-09-14', 'Projection1', \n",
    "    ['Datum_probe', 'Max_force'], 0.4648, 0.5664\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "99e4a2a19c08b165697a52cd79a4815ca9063c5131d2540ea9514263faee8422"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
